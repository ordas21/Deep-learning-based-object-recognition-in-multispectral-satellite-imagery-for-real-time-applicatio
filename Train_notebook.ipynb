{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB8EpebgJamz"
      },
      "outputs": [],
      "source": [
        "## Initialise libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dWUXhY3_Jam0",
        "outputId": "6a30ddea-6aa9-4f2d-ce1d-a305f453136e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from keras.models import model_from_json\n",
        "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from shapely.wkt import loads as wkt_loads\n",
        "import tifffile as tiff\n",
        "import os, sys\n",
        "import random\n",
        "from keras.models import Model\n",
        "from tensorflow.python.keras import Sequential\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout\n",
        "from keras.layers import concatenate\n",
        "#from keras.layers import LeakyReLU\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import PReLU, ThresholdedReLU\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import jaccard_score\n",
        "import skimage.transform\n",
        "import skimage.morphology\n",
        "from shapely.geometry import MultiPolygon, Polygon\n",
        "import shapely.wkt\n",
        "import shapely.affinity\n",
        "from collections import defaultdict\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from scipy.ndimage.interpolation import rotate as rotate\n",
        "import time\n",
        "#import tempfile\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGH3Yf-cJam1"
      },
      "source": [
        "## Initialies parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KhThyOU6Jam2",
        "outputId": "bae45458-c899-4a6d-f445-62818d92022f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'AOI_train_125/input_data/grid_sizes_125.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m temp_weights \u001b[39m=\u001b[39m Data_Folder \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/trained_models_v4/v_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(version_)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_unet_tmp_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(epochs) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mep_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(train_size) \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39msiz_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(ba_size)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mba_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(ISZ) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mICZ_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(Part_Proc) \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mproc\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.hdf5\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     35\u001b[0m Log_Dir \u001b[39m=\u001b[39m Data_Folder \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/log_data_v4/v_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(version_) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m activation_f1str \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_20191216/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 36\u001b[0m GS_s \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(Data_Folder \u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/input_data/grid_sizes_125.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mImageId\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mXmin\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mXmax\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mYmin\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mYmax\u001b[39;49m\u001b[39m'\u001b[39;49m], skiprows\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     37\u001b[0m DF_s \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(Data_Folder \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/input_data/combined_polygons_125_v4.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m Big_x \u001b[39m=\u001b[39m Data_Folder \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/composed_data_v4/all_x_trn_3_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ml_shit/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ml_shit/lib/python3.10/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ml_shit/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ml_shit/lib/python3.10/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ml_shit/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m     f,\n\u001b[1;32m   1218\u001b[0m     mode,\n\u001b[1;32m   1219\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1220\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1223\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1224\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1225\u001b[0m )\n\u001b[1;32m   1226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/ml_shit/lib/python3.10/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AOI_train_125/input_data/grid_sizes_125.csv'"
          ]
        }
      ],
      "source": [
        "img_count = 10   # po kiek pav imsim mokymui 3 - reiskia 3x3; 5 - 5x5\n",
        "test_img_count = 5\n",
        "img_size = 1300  # koks apmokymui skirtu pav. dydis pikseliais\n",
        "\n",
        "N_Cls = 1 #10\n",
        "N_bands = 3\n",
        "N_ToPredict = 200\n",
        "\n",
        "#hyperparamentrai\n",
        "epochs = 30\n",
        "train_size=25000\n",
        "val_size=0.2 #procentide dalis valicacijai\n",
        "ba_size = 128  # was 128\n",
        "#def leaky_relu(features, alpha=0.2, name=None) #Compute the Leaky ReLU activation function.\n",
        "def leaky_relu(x): \n",
        "    return LeakyReLU(alpha=0.1)(x)\n",
        "def prelu(x): \n",
        "    return PReLU()(x)\n",
        "def threshrelu(x): \n",
        "    return ThresholdedReLU()(x)\n",
        "\n",
        "activation_f1 = 'relu'\n",
        "activation_f1str = 'relu'\n",
        "activation_f2 = 'sigmoid'\n",
        "ISZ = 192 \n",
        "Part_Proc = 0.02\n",
        "pixel_threshold = 100 \n",
        "mask_overlap_koef = 0.25\n",
        "version_ = '4.1.3' #data.model.experiment\n",
        "Data_Folder = 'AOI_train_125';\n",
        "weights_spacenet = Data_Folder + '/trained_models_v4/v_'+str(version_)+'_unet_1_jk_' + str(epochs) + 'ep_' + str(train_size) +'siz_'+ str(ba_size)+'ba_'+str(ISZ) + 'ICZ_'+ str(Part_Proc) +'proc';  \n",
        "model_file = Data_Folder + '/trained_models_v4/v_'+str(version_)+'_model_spacenet_'+ str(epochs) + 'ep_' + str(train_size) +'siz_'+ str(ba_size)+'ba_'+str(ISZ) + 'ICZ_' + str(Part_Proc) +'proc'+'.json'; \n",
        "model_pb = Data_Folder + '/trained_models_v4/v_'+str(version_)+'_model_spacenet_'+ str(epochs) + 'ep_' + str(train_size) +'siz_'+ str(ba_size)+'ba_'+str(ISZ) + 'ICZ_' + str(Part_Proc) +'proc'+'.pb';\n",
        "temp_weights = Data_Folder +'/trained_models_v4/v_'+str(version_)+'_unet_tmp_' +str(epochs) + 'ep_' + str(train_size) +'siz_'+ str(ba_size)+'ba_'+str(ISZ) + 'ICZ_'+ str(Part_Proc) +'proc' +'.hdf5'\n",
        "Log_Dir = Data_Folder +'/log_data_v4/v_'+str(version_) + '_' + activation_f1str +'_20191216/'\n",
        "GS_s = pd.read_csv(Data_Folder +'/input_data/grid_sizes_125.csv', names=['ImageId','Xmin', 'Xmax', 'Ymin', 'Ymax'], skiprows=1)\n",
        "DF_s = pd.read_csv(Data_Folder +'/input_data/combined_polygons_125_v4.csv')\n",
        "\n",
        "Big_x = Data_Folder + '/composed_data_v4/all_x_trn_3_%d'\n",
        "Big_y = Data_Folder + '/composed_data_v4/all_y_trn_3_%d'\n",
        "\n",
        "Big_test_x = Data_Folder + '/composed_data_v4/all_x_test_3_%d'\n",
        "Big_test_y = Data_Folder + '/composed_data_v4/all_y_test_3_%d'\n",
        "\n",
        "patches_file_x = Data_Folder +'/composed_data_v4/x_data_v_4_patches_v_6_ISZ192_25000.npz'\n",
        "patches_file_y = Data_Folder +'/composed_data_v4/y_data_v_4_patches_v_6_ISZ192_25000.npz'\n",
        "\n",
        "smooth = 1e-12\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaMToRoOJam2"
      },
      "source": [
        "## Define all functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zdluosuJam3"
      },
      "outputs": [],
      "source": [
        "def timeit(method):\n",
        "    def timed(*args, **kw):\n",
        "        ts = time.time()\n",
        "        result = method(*args, **kw)\n",
        "        te = time.time()\n",
        "        if 'log_time' in kw:\n",
        "            name = kw.get('log_name', method.__name__.upper())\n",
        "            kw['log_time'][name] = int((te - ts) * 1000)\n",
        "        else:\n",
        "            print ('%2.2f' % \\\n",
        "                 ((te - ts) * 1000,))\n",
        "        return result\n",
        "    return timed\n",
        "\n",
        "def _convert_coordinates_to_raster_spacenet(coords, img_size, xyminmax):\n",
        "    Xmin, Xmax, Ymin, Ymax = xyminmax\n",
        "    H, W = img_size\n",
        "    coords[:, 1] = H - (coords[:, 1] - Ymin)/(Ymax-Ymin)*H\n",
        "    coords[:, 0] = (coords[:, 0] - Xmin)/(Xmax-Xmin)*W\n",
        "    coords_int = np.round(coords).astype(np.int32)\n",
        "    return coords_int\n",
        "\n",
        "\n",
        "def _get_xmin_xmax_ymin_ymax_spacenet(grid_sizes_panda, imageId):\n",
        "    xmin, xmax, ymin, ymax = grid_sizes_panda[grid_sizes_panda.ImageId == imageId].iloc[0, 1:].astype(float)\n",
        "    return (xmin,xmax, ymin, ymax)\n",
        "\n",
        "\n",
        "def _get_polygon_list(wkt_list_pandas, imageId, cType):\n",
        "    df_image = wkt_list_pandas[wkt_list_pandas.ImageId == imageId]\n",
        "    multipoly_def = df_image[df_image.ClassType == cType].MultipolygonWKT\n",
        "    polygonList = None\n",
        "    if len(multipoly_def) > 0:\n",
        "        assert len(multipoly_def) == 1\n",
        "        polygonList = wkt_loads(multipoly_def.values[0])\n",
        "    return polygonList\n",
        "\n",
        "def _get_and_convert_contours_spacenet(polygonList, raster_img_size, xyminmax):\n",
        "    perim_list = []\n",
        "    interior_list = []\n",
        "    if polygonList is None:\n",
        "        return None\n",
        "    for k in range(len(polygonList)):\n",
        "        poly = polygonList[k]\n",
        "        perim = np.array(list(poly.exterior.coords))\n",
        "        perim_c = _convert_coordinates_to_raster_spacenet(perim, raster_img_size, xyminmax)\n",
        "        if not any(np.array_equal(perim_c, x) for x in perim_list):\n",
        "            perim_list.append(perim_c)\n",
        "            for pi in poly.interiors:\n",
        "                interior = np.array(list(pi.coords))\n",
        "                interior_c = _convert_coordinates_to_raster_spacenet(interior, raster_img_size, xyminmax)\n",
        "                interior_list.append(interior_c)\n",
        "    return perim_list, interior_list\n",
        "\n",
        "def _plot_mask_from_contours(raster_img_size, contours, class_value=1):\n",
        "    img_mask = np.zeros(raster_img_size, np.uint8)\n",
        "    if contours is None:\n",
        "        return img_mask\n",
        "    perim_list, interior_list = contours\n",
        "    cv2.fillPoly(img_mask, perim_list, class_value)\n",
        "    cv2.fillPoly(img_mask, interior_list, 0)\n",
        "    return img_mask\n",
        "\n",
        "\n",
        "def generate_mask_for_image_and_class_spacenet(raster_size, imageId, class_type, grid_sizes_panda=GS_s, wkt_list_pandas=DF_s):\n",
        "    xyminmax = _get_xmin_xmax_ymin_ymax_spacenet(grid_sizes_panda, imageId)\n",
        "    polygon_list = _get_polygon_list(wkt_list_pandas, imageId, class_type)\n",
        "    contours = _get_and_convert_contours_spacenet(polygon_list, raster_size, xyminmax)\n",
        "    mask = _plot_mask_from_contours(raster_size, contours, 1)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def M_spacenet(image_id):\n",
        "    img = tiff.imread('AOI_train_125/input_data/{}.tif'.format(image_id))\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    return img\n",
        "\n",
        "def M_spacenet_small(image_id):\n",
        "    img = tiff.imread('AOI_train_125/input_data/{}.tif'.format(image_id))\n",
        "    img = cv2.resize(img, (ISZ, ISZ))\n",
        "    return img\n",
        "\n",
        "\n",
        "def stretch_n(bands, lower_percent=0, higher_percent=100):\n",
        "    out = np.zeros_like(bands, dtype=np.float32)\n",
        "    n = bands.shape[2]\n",
        "    for i in range(n):\n",
        "        a = 0\n",
        "        b = 1\n",
        "        c = np.percentile(bands[:, :, i], lower_percent)\n",
        "        d = np.percentile(bands[:, :, i], higher_percent)\n",
        "        t = a + (bands[:, :, i] - c) * (b - a) / (d - c)\n",
        "        t[t < a] = a\n",
        "        t[t > b] = b\n",
        "        out[:, :, i] = t\n",
        "\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "\n",
        "def jaccard_coef(y_true, y_pred):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
        "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
        "\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    rez=K.mean(jac)\n",
        "    return rez\n",
        "\n",
        "\n",
        "def jaccard_coef_int(y_true, y_pred):\n",
        "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
        "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    rez=K.mean(jac)\n",
        "    return rez\n",
        "\n",
        "\n",
        "def stick_all_train_spacenet():\n",
        "    print(\"Stick all imgs together:\")\n",
        "    s = img_size\n",
        "\n",
        "    x = np.zeros((img_count * s, img_count * s, N_bands))\n",
        "    y = np.zeros((img_count * s, img_count * s, N_Cls))\n",
        "    \n",
        "    x_test = np.zeros((test_img_count * s, test_img_count * s, N_bands))\n",
        "    y_test = np.zeros((test_img_count * s, test_img_count * s, N_Cls))\n",
        "    \n",
        "    ids_all = sorted(DF_s.ImageId.unique())\n",
        "    random.shuffle(ids_all)\n",
        "    ids = ids_all[:img_count*img_count]\n",
        "    ids_test = ids_all[-(test_img_count*test_img_count):]\n",
        "    del ids_all\n",
        "    \n",
        "    for i in range(img_count):\n",
        "        for j in range(img_count):\n",
        "            id_ = ids[img_count * i + j]\n",
        "            print(\"train id\", id_)\n",
        "            img = M_spacenet(id_)\n",
        "            img = stretch_n(img)\n",
        "            x[s * i:s * i + s, s * j:s * j + s, :] = img[:s, :s, :]\n",
        "            for z in range(N_Cls):\n",
        "                y[s * i:s * i + s, s * j:s * j + s, z] = generate_mask_for_image_and_class_spacenet(\n",
        "                    (img.shape[0], img.shape[1]), id_, z + 1)[:s, :s]\n",
        "    np.save(Big_x % N_Cls, x)\n",
        "    np.save(Big_y % N_Cls, y)\n",
        "    \n",
        "    for i in range(test_img_count):\n",
        "        for j in range(test_img_count):\n",
        "            id_ = ids_test[test_img_count * i + j]\n",
        "            print(\"test id\", id_)\n",
        "            img = M_spacenet(id_)\n",
        "            img = stretch_n(img)\n",
        "            x_test[s * i:s * i + s, s * j:s * j + s, :] = img[:s, :s, :]\n",
        "            for z in range(N_Cls):\n",
        "                y_test[s * i:s * i + s, s * j:s * j + s, z] = generate_mask_for_image_and_class_spacenet(\n",
        "                    (img.shape[0], img.shape[1]), id_, z + 1)[:s, :s]\n",
        "    \n",
        "    np.save(Big_test_x % N_Cls, x_test)\n",
        "    np.save(Big_test_y % N_Cls, y_test)\n",
        "\n",
        "    \n",
        "def stick_all_train_spacenet_v2():\n",
        "    print(\"Stick all imgs together:\")\n",
        "    sx = img_size\n",
        "    sy = img_size\n",
        "    x = np.zeros((img_count * sx, img_count * sy, N_bands))\n",
        "    y = np.zeros((img_count * sx, img_count * sy, N_Cls))\n",
        "    \n",
        "    x_test = np.zeros((test_img_count * sx, test_img_count * sy, N_bands))\n",
        "    y_test = np.zeros((test_img_count * sx, test_img_count * sy, N_Cls))\n",
        "    \n",
        "    ids_all = sorted(DF_s.ImageId.unique())\n",
        "    random.shuffle(ids_all)\n",
        "    print(ids_all)\n",
        "    counter = 0\n",
        "    for id_ in ids_all:\n",
        "        counter += 1\n",
        "        img = M_spacenet(id_)\n",
        "        img = stretch_n(img)\n",
        "        if counter <= 50:\n",
        "            s = img_count * 2\n",
        "            sy = int( img_size / 2 )\n",
        "            i = int((counter - 1) / s)\n",
        "            j = ((counter - 1) % s)\n",
        "        else:\n",
        "            s = img_count\n",
        "            sy = img_size\n",
        "            i = int((counter - 16) / s) - 1\n",
        "            j = (counter + 4) % s\n",
        "        x[sx * i:sx * i + sx, sy * j:sy * j + sy, :] = img[:sx, :sy, :]    \n",
        "        for z in range(N_Cls):\n",
        "            y[sx * i:sx * i + sx, sy * j:sy * j + sy, z] = generate_mask_for_image_and_class_spacenet(\n",
        "                (img.shape[0], img.shape[1]), id_, z + 1)[:sx, :sy]\n",
        "        \n",
        "        if counter <= 50:\n",
        "            s = test_img_count * 2\n",
        "            sy = int( img_size / 2 )\n",
        "            i = int((counter - 1) / s)\n",
        "            j = ((counter - 1) % s)\n",
        "            x_test[sx * i:sx * i + sx, sy * j:sy * j + sy, :] = img[:sx, sy:, :]\n",
        "            for z in range(N_Cls):\n",
        "                y_test[sx * i:sx * i + sx, sy * j:sy * j + sy, z] = generate_mask_for_image_and_class_spacenet(\n",
        "                    (img.shape[0], img.shape[1]), id_, z + 1)[:sx, sy:] \n",
        "    del ids_all\n",
        "    np.save(Big_x % N_Cls, x)\n",
        "    np.save(Big_y % N_Cls, y)\n",
        "    np.save(Big_test_x % N_Cls, x_test)\n",
        "    np.save(Big_test_y % N_Cls, y_test)\n",
        "    \n",
        "def random_brightness(image):\n",
        "    image = cv2.cvtColor(image.astype(np.float32), cv2.COLOR_BGR2RGB)    \n",
        "    # Convert 2 HSV colorspace from BGR colorspace\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    # Generate new random brightness\n",
        "    rand = random.uniform(0.5, 1.0)\n",
        "    hsv[:, :, 2] = rand*hsv[:, :, 2]\n",
        "    # Convert back to BGR colorspace\n",
        "    new_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "    return new_img\n",
        "\n",
        "\n",
        "def rotate_image(img, angle, pivot):\n",
        "    padX = [img.shape[1] - pivot[0], pivot[0]]\n",
        "    padY = [img.shape[0] - pivot[1], pivot[1]]\n",
        "    imgP = np.pad(img, [padY, padX, [0, 0]], 'constant')\n",
        "    imgR = ndimage.rotate(imgP, angle, reshape=False)\n",
        "    return imgR[padY[0] : -padY[1], padX[0] : -padX[1]]\n",
        "\n",
        "\n",
        "def rotate_patch_image(img, msk):\n",
        "    pivot = (int(ISZ/2), int(ISZ/2))\n",
        "    angle = random.randrange(1, 360, 1)\n",
        "    rot_img = rotate_image(img, angle, pivot)\n",
        "    rot_msk = rotate_image(msk, angle, pivot)\n",
        "    return rot_img, rot_msk\n",
        "\n",
        "\n",
        "def add_noise(img):\n",
        "    # ADDING NOISE\n",
        "    noise = np.random.uniform(-0.05, 0.05, size = (img.shape[0], img.shape[1], 3))\n",
        "    img = img + noise\n",
        "    img[img>1] = 1\n",
        "    img[img<0] = 0\n",
        "    return img\n",
        "\n",
        "\n",
        "def add_noise_to_color(img):\n",
        "    noise = np.random.uniform(-0.15, 0.15)\n",
        "    ch = np.random.randint(0,3)\n",
        "    img[:,:,ch] = img[:,:,ch] + noise\n",
        "    img[img>1] = 1\n",
        "    img[img<0] = 0\n",
        "    return img\n",
        "\n",
        "\n",
        "def validate_coord(xc,yc):\n",
        "    is2 = int(1.0 * ISZ)\n",
        "    if xc < img_size * 2:\n",
        "        if img_size - (xc % img_size) < is2:\n",
        "            return False\n",
        "        if ((img_size/2) - (yc % (img_size/2))) < is2:\n",
        "            return False\n",
        "    elif xc > img_size * 2 and xc < img_size * 3 and yc < img_size * 5:\n",
        "        if img_size - (xc % img_size) < is2:\n",
        "            return False\n",
        "        if ((img_size/2) - (yc % (img_size/2))) < is2:\n",
        "            return False\n",
        "    else:    \n",
        "        if img_size - (xc % img_size) < is2:\n",
        "            return False\n",
        "        elif img_size - (yc % img_size) < is2:\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def get_x_patches_from_file():\n",
        "    print(\"Loading x patches from file\")\n",
        "    data1 = np.load(patches_file_x)\n",
        "    return data1['x']\n",
        "\n",
        "def get_patches_from_file():\n",
        "    print(\"Loading patches from file\")\n",
        "    data1 = np.load(patches_file_x)\n",
        "    data2 = np.load(patches_file_y)\n",
        "    return data1['x'], data2['y']\n",
        "    \n",
        "def get_patches_spacenet(img, msk, amt, aug=True, show_image=False):\n",
        "    is2 = int(1.0 * ISZ)\n",
        "    xm, ym = img.shape[0], img.shape[1]\n",
        "\n",
        "    if show_image:\n",
        "        fig, ax1 = plt.subplots(figsize=(60,40),facecolor='w')\n",
        "        ax1.set_title('Big Image')\n",
        "        plt.imshow(msk[:,:,0], cmap=plt.get_cmap('gray'))\n",
        "    \n",
        "    x, y = [], []\n",
        "    coords_arr = []\n",
        "    coords_arr2 = []\n",
        "    skipped = 0\n",
        "    tr = [Part_Proc]\n",
        "    skip_limit = 15000\n",
        "    while len(x) < amt and skipped < skip_limit:\n",
        "        xc = random.randint(0, xm)\n",
        "        yc = random.randint(0, ym)\n",
        "        if not validate_coord(xc,yc):\n",
        "            skipped += 1\n",
        "            continue\n",
        "            \n",
        "        coord = str(xc)+\"_\"+str(yc)\n",
        "        if coord in coords_arr:\n",
        "            skipped += 1\n",
        "            print (\"Total skipped images:\", skipped, \" Total:\", len(x))\n",
        "            continue\n",
        "        else:\n",
        "            coords_arr.append(coord) \n",
        "\n",
        "        im = img[xc:xc + is2, yc:yc + is2]\n",
        "        ms = msk[xc:xc + is2, yc:yc + is2]\n",
        "        for j in range(N_Cls):        \n",
        "            sm = np.sum(ms[:, :, j])\n",
        "            if ((1.0 * sm) / (is2 ** 2)) > tr[j]: \n",
        "                coords_arr2.append(coord) \n",
        "                x.append(im.astype(np.float32))\n",
        "                y.append(ms.astype(np.float32))\n",
        "                \n",
        "                if aug and (len(x) + 6 < amt):\n",
        "                    if random.uniform(0, 1) > 0.9:\n",
        "                        im_aug = im[::-1]\n",
        "                        ms_aug = ms[::-1]\n",
        "                        x.append(im_aug.astype(np.float32))\n",
        "                        y.append(ms_aug.astype(np.float32))\n",
        "                    if random.uniform(0, 1) > 0.9:\n",
        "                        im_aug = im[:, ::-1]\n",
        "                        ms_aug = ms[:, ::-1]\n",
        "                        x.append(im_aug.astype(np.float32))\n",
        "                        y.append(ms_aug.astype(np.float32))\n",
        "                    if random.uniform(0, 1) > 0.7:\n",
        "                        im_aug = random_brightness(im)\n",
        "                        x.append(im_aug.astype(np.float32))\n",
        "                        y.append(ms.astype(np.float32))                        \n",
        "                    if random.uniform(0, 1) > 0.7:    \n",
        "                        im_aug = add_noise(im)\n",
        "                        x.append(im_aug.astype(np.float32))\n",
        "                        y.append(ms.astype(np.float32))\n",
        "                    if random.uniform(0, 1) > 0.8:    \n",
        "                        im_aug = add_noise_to_color(im)\n",
        "                        x.append(im_aug.astype(np.float32))\n",
        "                        y.append(ms.astype(np.float32))\n",
        "                    if random.uniform(0, 1) > 1:\n",
        "                        im_aug, ms_aug = rotate_patch_image(im, ms)\n",
        "                        im_aug = stretch_n(im_aug)\n",
        "                        ms_aug = np.round(ms_aug, 0)\n",
        "                        x.append(im_aug.astype(np.float32))\n",
        "                        y.append(ms_aug.astype(np.float32))\n",
        "\n",
        "                if show_image:\n",
        "                    rect = patches.Rectangle((yc,xc), is2, is2,linewidth=2,edgecolor='r',facecolor='none', fill=False)\n",
        "                    ax1.add_patch(rect)\n",
        "                    \n",
        "                if len(x) % 100 == 0:\n",
        "                    print(\" Working: \", len(x), \"/\", amt)\n",
        "    if show_image:\n",
        "        plt.show() \n",
        "    \n",
        "    x, y = 2 * np.transpose(x, (0, 3, 1, 2)) - 1, np.transpose(y, (0, 3, 1, 2))\n",
        "    print(x.shape, y.shape, np.amax(x), np.amin(x), np.amax(y), np.amin(y), type(x[0]), type(np.amin(x)))\n",
        "    return x, y \n",
        "\n",
        "\n",
        "\n",
        "def make_val_spacenet():\n",
        "    print(\"let's pick some samples for validation\")\n",
        "    img = np.load('AOI_train_81/composed_data/x_trn_3_%d.npy' % N_Cls)\n",
        "    msk = np.load('AOI_train_81/composed_data/y_trn_3_%d.npy' % N_Cls)\n",
        "    x, y = get_patches_spacenet(img, msk, amt=3000)\n",
        "\n",
        "    np.save('AOI_train_81/composed_data/x_tmp_3_%d'  % N_Cls, x)\n",
        "    np.save('AOI_train_81/composed_data/y_tmp_3_%d'  % N_Cls, y)\n",
        "\n",
        "\n",
        "def get_unet_2_old():\n",
        "    inputs = Input((N_bands, ISZ, ISZ))\n",
        "    conv1 = Conv2D(8, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(inputs)\n",
        "    conv1 = Conv2D(8, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv1)\n",
        "\n",
        "    conv2 = Conv2D(16, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool1)\n",
        "    conv2 = Conv2D(16, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv2)\n",
        "\n",
        "    conv3 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool2)\n",
        "    conv3 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv3)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv3), conv2], axis=1)\n",
        "    conv8 = Conv2D(16, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up8)\n",
        "    conv8 = Conv2D(16, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv8), conv1], axis=1)\n",
        "    conv9 = Conv2D(8, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up9)\n",
        "    conv9 = Conv2D(8, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv9)\n",
        "\n",
        "    conv10 = Conv2D(N_Cls, (1, 1), activation=activation_f2, data_format=\"channels_first\")(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv10)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[jaccard_coef, jaccard_coef_int, 'accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_unet_1():\n",
        "    inputs = Input((N_bands, ISZ, ISZ))\n",
        "    conv1 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv4)\n",
        "    \n",
        "    up5 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv4), conv3], axis=1)\n",
        "    conv5 = Conv2D(128, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up5)\n",
        "    conv5 = Conv2D(128, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv5)\n",
        "\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv5), conv2], axis=1)\n",
        "    conv6 = Conv2D(64, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up6)\n",
        "    conv6 = Conv2D(64, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv6), conv1], axis=1)\n",
        "    conv7 = Conv2D(32, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up7)\n",
        "    conv7 = Conv2D(32, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv7)\n",
        "\n",
        "    conv8 = Conv2D(N_Cls, (1, 1), activation=activation_f2, data_format=\"channels_first\")(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv8)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[jaccard_coef, jaccard_coef_int, 'accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_unet_2():\n",
        "    inputs = Input((N_bands, ISZ, ISZ))\n",
        "    conv1 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv5)\n",
        "   \n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv5), conv4], axis=1)\n",
        "    conv6 = Conv2D(256, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv6), conv3], axis=1)\n",
        "    conv7 = Conv2D(128, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv7), conv2], axis=1)\n",
        "    conv8 = Conv2D(64, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv8), conv1], axis=1)\n",
        "    conv9 = Conv2D(32, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv9)\n",
        "\n",
        "    conv10 = Conv2D(N_Cls, (1, 1), activation=activation_f2, data_format=\"channels_first\")(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv10)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[jaccard_coef, jaccard_coef_int, 'accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_unet_3():\n",
        "    inputs = Input((N_bands, ISZ, ISZ))\n",
        "    conv1 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv5)\n",
        "    pool5 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv5)   \n",
        "        \n",
        "    conv6 = Conv2D(1024, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool5)\n",
        "    conv6 = Conv2D(1024, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv6)\n",
        "    \n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv6), conv5], axis=1)\n",
        "    conv7 = Conv2D(512, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up7)\n",
        "    conv7 = Conv2D(512, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv7) \n",
        "   \n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv7), conv4], axis=1)\n",
        "    conv8 = Conv2D(256, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up8)\n",
        "    conv8 = Conv2D(256, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv8), conv3], axis=1)\n",
        "    conv9 = Conv2D(128, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up9)\n",
        "    conv9 = Conv2D(128, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv9)\n",
        "\n",
        "    up10 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv9), conv2], axis=1)\n",
        "    conv10 = Conv2D(64, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up10)\n",
        "    conv10 = Conv2D(64, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv10)\n",
        "\n",
        "    up11 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv10), conv1], axis=1)\n",
        "    conv11 = Conv2D(32, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up11)\n",
        "    conv11 = Conv2D(32, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv11)\n",
        "\n",
        "    conv12 = Conv2D(N_Cls, (1, 1), activation=activation_f2, data_format=\"channels_first\")(conv11)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv12)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[jaccard_coef, jaccard_coef_int, 'accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_unet_4():\n",
        "    inputs = Input((N_bands, ISZ, ISZ))\n",
        "    conv1 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv5)\n",
        "    pool5 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv5)   \n",
        " \n",
        "    conv6 = Conv2D(1024, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool5)\n",
        "    conv6 = Conv2D(1024, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv6)\n",
        "    pool6 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(conv6)        \n",
        "    \n",
        "    conv7 = Conv2D(2048, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(pool6)\n",
        "    conv7 = Conv2D(2048, (3, 3), activation=activation_f1, padding='same', data_format=\"channels_first\")(conv7)\n",
        "    \n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv7), conv6], axis=1)\n",
        "    conv8 = Conv2D(1024, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up8)\n",
        "    conv8 = Conv2D(1024, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv8)    \n",
        "    \n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv8), conv5], axis=1)\n",
        "    conv9 = Conv2D(512, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up9)\n",
        "    conv9 = Conv2D(512, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv9) \n",
        "   \n",
        "    up10 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv9), conv4], axis=1)\n",
        "    conv10 = Conv2D(256, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up10)\n",
        "    conv10 = Conv2D(256, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv10)\n",
        "\n",
        "    up11 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv10), conv3], axis=1)\n",
        "    conv11 = Conv2D(128, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up11)\n",
        "    conv11 = Conv2D(128, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv11)\n",
        "\n",
        "    up12 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv11), conv2], axis=1)\n",
        "    conv12 = Conv2D(64, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up12)\n",
        "    conv12 = Conv2D(64, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv12)\n",
        "\n",
        "    up13 = concatenate([UpSampling2D(size=(2, 2), data_format=\"channels_first\")(conv12), conv1], axis=1)\n",
        "    conv13 = Conv2D(32, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(up13)\n",
        "    conv13 = Conv2D(32, (3, 3), activation=activation_f1, padding='same',  data_format=\"channels_first\")(conv13)\n",
        "\n",
        "    conv14 = Conv2D(N_Cls, (1, 1), activation=activation_f2, data_format=\"channels_first\")(conv13)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv14)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[jaccard_coef, jaccard_coef_int, 'accuracy'])\n",
        "    return model\n",
        "\n",
        "def calc_jacc(model):\n",
        "    img = np.load('data/x_tmp_3_%d.npy' % N_Cls)\n",
        "    msk = np.load('data/y_tmp_3_%d.npy' % N_Cls)\n",
        "\n",
        "    prd = model.predict(img, batch_size=4)\n",
        "    print(prd.shape, msk.shape)\n",
        "    avg, trs = [], []\n",
        "\n",
        "    for i in range(N_Cls):\n",
        "        t_msk = msk[:, i, :, :]\n",
        "        t_prd = prd[:, i, :, :]\n",
        "        t_msk = t_msk.reshape(msk.shape[0] * msk.shape[2], msk.shape[3])\n",
        "        t_prd = t_prd.reshape(msk.shape[0] * msk.shape[2], msk.shape[3])\n",
        "\n",
        "        m, b_tr = 0, 0\n",
        "        for j in range(10):\n",
        "            tr = j / 10.0\n",
        "            pred_binary_mask = t_prd > tr\n",
        "\n",
        "            jk = jaccard_score(t_msk, pred_binary_mask)\n",
        "            if jk > m:\n",
        "                m = jk\n",
        "                b_tr = tr\n",
        "        print(i, m, b_tr)\n",
        "        avg.append(m)\n",
        "        trs.append(b_tr)\n",
        "\n",
        "    score = sum(avg) / 10.0\n",
        "    return score, trs\n",
        "\n",
        "\n",
        "def calc_jacc_spacenet(model):\n",
        "    img = np.load('composed_data_v4/x_tmp_3_%d.npy' % N_Cls)\n",
        "    msk = np.load('composed_data_v4/y_tmp_3_%d.npy' % N_Cls)\n",
        "\n",
        "    prd = model.predict(img, batch_size=4)\n",
        "    avg, trs = [], []\n",
        "\n",
        "    for i in range(N_Cls):\n",
        "        t_msk = msk[:, i, :, :]\n",
        "        t_prd = prd[:, i, :, :]\n",
        "        t_msk = t_msk.reshape(msk.shape[0] * msk.shape[2], msk.shape[3])\n",
        "        t_prd = t_prd.reshape(msk.shape[0] * msk.shape[2], msk.shape[3])\n",
        "\n",
        "        m, b_tr = 0, 0\n",
        "        for j in range(10):\n",
        "            tr = j / 10.0\n",
        "            pred_binary_mask = t_prd > tr\n",
        "\n",
        "            jk = jaccard_score(t_msk, pred_binary_mask)\n",
        "            if jk > m:\n",
        "                m = jk\n",
        "                b_tr = tr\n",
        "        print(i, m, b_tr)\n",
        "        avg.append(m)\n",
        "        trs.append(b_tr)\n",
        "\n",
        "    score = sum(avg) / 10.0\n",
        "    return score, trs\n",
        "\n",
        "\n",
        "\n",
        "def mask_for_polygons(polygons, im_size):\n",
        "    img_mask = np.zeros(im_size, np.uint8)\n",
        "    if not polygons:\n",
        "        return img_mask\n",
        "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
        "    exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
        "    interiors = [int_coords(pi.coords) for poly in polygons\n",
        "                 for pi in poly.interiors]\n",
        "    cv2.fillPoly(img_mask, exteriors, 1)\n",
        "    cv2.fillPoly(img_mask, interiors, 0)\n",
        "    return img_mask\n",
        "\n",
        "\n",
        "def mask_to_polygons(mask, epsilon=1, min_area=1.):\n",
        "    image, contours, hierarchy = cv2.findContours(\n",
        "        ((mask == 1) * 255).astype(np.uint8),\n",
        "        cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_KCOS)\n",
        "    approx_contours = [cv2.approxPolyDP(cnt, epsilon, True)\n",
        "                       for cnt in contours]\n",
        "    if not contours:\n",
        "        return MultiPolygon()\n",
        "    cnt_children = defaultdict(list)\n",
        "    child_contours = set()\n",
        "    assert hierarchy.shape[0] == 1\n",
        "    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n",
        "        if parent_idx != -1:\n",
        "            child_contours.add(idx)\n",
        "            cnt_children[parent_idx].append(approx_contours[idx])\n",
        "    all_polygons = []\n",
        "    for idx, cnt in enumerate(approx_contours):\n",
        "        if idx not in child_contours and cv2.contourArea(cnt) >= min_area:\n",
        "            assert cnt.shape[1] == 1\n",
        "            poly = Polygon(\n",
        "                shell=cnt[:, 0, :],\n",
        "                holes=[c[:, 0, :] for c in cnt_children.get(idx, [])\n",
        "                       if cv2.contourArea(c) >= min_area])\n",
        "            all_polygons.append(poly)\n",
        "    all_polygons = MultiPolygon(all_polygons)\n",
        "    if not all_polygons.is_valid:\n",
        "        all_polygons = all_polygons.buffer(0)\n",
        "        if all_polygons.type == 'Polygon':\n",
        "            all_polygons = MultiPolygon([all_polygons])\n",
        "    return all_polygons\n",
        "\n",
        "def train_net_spacenet():\n",
        "    print(\"start train net: get patches\")\n",
        "    x_trn, y_trn = get_patches_from_file()\n",
        "    print('Patches collected:')\n",
        "    model = get_unet_4()\n",
        "    print(\"start train net: start fit\")\n",
        "    model_checkpoint = ModelCheckpoint(temp_weights, monitor='loss', save_best_only=True)\n",
        "    model_TensorBoard = TensorBoard(log_dir=Log_Dir, histogram_freq=0, batch_size=ba_size, write_graph=False, write_grads=False,\n",
        "                                    write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, \n",
        "                                    embeddings_data=None)\n",
        "\n",
        "    for i in range(1):\n",
        "        model.fit(x_trn, y_trn, validation_split=val_size, batch_size=ba_size, epochs=epochs, verbose=1,\n",
        "                  callbacks=[model_checkpoint, model_TensorBoard], shuffle=True)\n",
        "        del x_trn\n",
        "        del y_trn\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_id_spacenet(id, model):\n",
        "    img = M_spacenet(id)\n",
        "    x = stretch_n(img)\n",
        "\n",
        "    cnv = np.zeros((img_size, img_size, N_bands)).astype(np.float32)\n",
        "    prd = np.zeros((N_Cls, img_size, img_size)).astype(np.float32)\n",
        "    cnv[:img.shape[0], :img.shape[1], :] = x\n",
        "\n",
        "    for i in range(0, 8):\n",
        "        line = []\n",
        "        for j in range(0, 8):\n",
        "            line.append(cnv[i * ISZ:(i + 1) * ISZ, j * ISZ:(j + 1) * ISZ])\n",
        "\n",
        "        x = 2 * np.transpose(line, (0, 3, 1, 2)) - 1\n",
        "        tmp = model.predict(x, batch_size=4)\n",
        "        for j in range(tmp.shape[0]):\n",
        "            prd[:, i * ISZ:(i + 1) * ISZ, j * ISZ:(j + 1) * ISZ] = tmp[j]\n",
        "\n",
        "    return prd[:, :img.shape[0], :img.shape[1]]\n",
        "\n",
        "def model_save_spacenet(model_send):\n",
        "    model_json = model_send.to_json()\n",
        "    json_file = open(model_file, \"w\")\n",
        "    json_file.write(model_json)\n",
        "    json_file.close()\n",
        "    model_send.save_weights(weights_spacenet)\n",
        "\n",
        "def model_load_spacenet():\n",
        "    json_file = open(model_file, \"r\")\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json, custom_objects={'prelu': activation_f1})\n",
        "    loaded_model.load_weights(weights_spacenet)\n",
        "    return loaded_model\n",
        "\n",
        "def calculate_mask_count(im):\n",
        "    labels,number_of_masks = ndimage.label(im)\n",
        "    areas = ndimage.sum(im, labels, range(1, labels.max()+1))\n",
        "    areas_threshold = np.where(areas > pixel_threshold)\n",
        "    areas_threshold = np.asarray(areas_threshold)\n",
        "    number_of_masks_th=areas_threshold.size\n",
        "    return (number_of_masks, number_of_masks_th)\n",
        "\n",
        "def compare_every_mask(pred_mask, true_mask):\n",
        "    labels,number_of_masks = ndimage.label(pred_mask)\n",
        "    slices = ndimage.find_objects(labels)\n",
        "\n",
        "    masks_nm = 0\n",
        "    found_objs = 0\n",
        "    for sl in slices:\n",
        "        sl_size = (pred_mask[sl] == 1).sum() \n",
        "        if sl_size >= pixel_threshold:\n",
        "            masks_nm += 1;\n",
        "            true_size = (true_mask[sl] == 1).sum()\n",
        "            if true_size > sl_size * mask_overlap_koef:\n",
        "                found_objs += 1\n",
        "                \n",
        "    print (found_objs, \"/\", masks_nm, \"/\", number_of_masks)\n",
        "    \n",
        "    return (1,1)\n",
        "\n",
        "def check_predict_spacenet(img_id,model):\n",
        "    print('\\start predict!!')\n",
        "    msk = predict_id_spacenet(img_id, model)\n",
        "    img=M_spacenet(img_id)\n",
        "    msk_true = np.zeros((img.shape[0], img.shape[1], N_Cls))\n",
        "    msk_true = generate_mask_for_image_and_class_spacenet((img_size, img_size), img_id, 1)\n",
        "   \n",
        "    print(\"check_predict_spacenet... 1 Mask shapes: \", msk_true.shape, msk.shape)\n",
        "\n",
        "    true_msk_count_no_thres, true_msk_count = calculate_mask_count(msk_true)\n",
        "    print('Number of true masks before threshold: ',true_msk_count_no_thres) \n",
        "    print('Number of true masks after threshold: ',true_msk_count_no_thres) \n",
        "    msk = np.squeeze(msk)\n",
        "    msk = np.round(msk, 0)\n",
        "    msk = remove_bad_pixels_from_mask(msk)\n",
        "    pred_msk_count_no_tres, pred_msk_count = calculate_mask_count(msk)\n",
        "    print('Number of predicted masks before threshold: ',pred_msk_count_no_tres) \n",
        "    print('Number of predicted  masks after threshold: ',pred_msk_count) \n",
        "          \n",
        "    plt.figure()\n",
        "    fig, ax1 = plt.subplots(figsize=(30,15),facecolor='w')  \n",
        "    ax1 = plt.subplot(131)\n",
        "    ax1.set_title('IMAGE')\n",
        "    ax2 = plt.subplot(132)\n",
        "    ax2.set_title('predict bldg pixels')\n",
        "    ax2.imshow(msk)\n",
        "    rez=jaccard_coef(msk_true, msk)\n",
        "    print('jaccard_coef: ',rez)\n",
        "\n",
        "    ax3 = plt.subplot(133)\n",
        "    ax3.set_title('true mask from file')\n",
        "    ax3.imshow(msk_true)\n",
        "    plt.show()\n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(60,40),facecolor='w')\n",
        "    plt.imshow(msk, cmap=plt.get_cmap('gray'))\n",
        "  \n",
        "    \n",
        "def show_image_true_predicted_mask(img_id):  #function for showing and testing\n",
        "      # --- image and true masks (begin) ---- \n",
        "    img = tiff.imread(Data_Folder+'/input_data/{}.tif'.format(img_id))\n",
        "    img = stretch_n(img)\n",
        "    msk_true = np.zeros((img.shape[0], img.shape[1], N_Cls))\n",
        "    msk_true = generate_mask_for_image_and_class_spacenet((img_size, img_size), img_id, 1)\n",
        "    print('img.size',img.shape[0]) \n",
        "    img_and_msk_true = img\n",
        "    for i in range(img.shape[0]):\n",
        "      for j in range(img.shape[1]):\n",
        "        if msk_true[i,j] == 1:\n",
        "            img_and_msk_true[i,j] = (1, 1 ,1)   \n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(60,40),facecolor='w')\n",
        "    plt.imshow(img_and_msk_true)\n",
        "    # --- image and true masks (end) ---- \n",
        "    \n",
        "    # --- image and predicted masks (begin) ----\n",
        "    model = model_load_spacenet()\n",
        "    msk_pred = predict_id_spacenet(img_id, model)\n",
        "    msk_pred = np.squeeze(msk_pred)\n",
        "    msk_pred = np.round(msk_pred, 0)\n",
        "    print('img.size',img.shape[0]) \n",
        "    img_and_msk_pred = img\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            if msk_pred[i,j] == 1:\n",
        "                img_and_msk_pred[i,j] = (1, 1 ,1)   \n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(60,40),facecolor='w')\n",
        "    plt.imshow(img_and_msk_pred)\n",
        "    # --- image and predicted masks (end) ----  \n",
        "\n",
        "    # --- image and predicted masks after pixel_threhold(begin) ----\n",
        "    model = model_load_spacenet()\n",
        "    msk_pred = predict_id_spacenet(img_id, model)\n",
        "    msk_pred = np.squeeze(msk_pred)\n",
        "    msk_pred = np.round(msk_pred, 0)\n",
        "    msk_pred = remove_bad_pixels_from_mask(msk_pred)\n",
        "    print('img.size',img.shape[0]) \n",
        "    img_and_msk_pred_thres = img\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            if msk_pred[i,j] == 1:\n",
        "                img_and_msk_pred_thres[i,j] = (1, 1 ,1)   \n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(60,40),facecolor='w')\n",
        "    plt.imshow(img_and_msk_pred_thres)\n",
        "  # --- image and predicted masks (end) ----  \n",
        "\n",
        "\n",
        "def show_true_and_predicted_mask(img_id):  #function for showing and testing\n",
        "      # --- image and true masks (begin) ---- \n",
        "    img = tiff.imread(Data_Folder+'/input_data/{}.tif'.format(img_id))\n",
        "    img = stretch_n(img)\n",
        "    msk_true = np.zeros((img.shape[0], img.shape[1], N_Cls))\n",
        "    msk_true = generate_mask_for_image_and_class_spacenet((img_size, img_size), img_id, 1)\n",
        "    fig, ax1 = plt.subplots(figsize=(60,40),facecolor='w')\n",
        "    plt.imshow(msk_true)\n",
        "    # --- image and true masks (end) ---- \n",
        "    \n",
        "    # --- image and predicted masks (begin) ----\n",
        "    model = model_load_spacenet()\n",
        "    msk_pred = predict_id_spacenet(img_id, model)\n",
        "    msk_pred = np.squeeze(msk_pred)\n",
        "    msk_pred = np.round(msk_pred, 0)\n",
        "       \n",
        "    fig, ax1 = plt.subplots(figsize=(60,40),facecolor='w')\n",
        "    plt.imshow(msk_pred)\n",
        "    # --- image and predicted masks (end) ----  \n",
        "\n",
        "    # --- image and predicted masks after pixel_threhold(begin) ----\n",
        "    msk_pred = remove_bad_pixels_from_mask(msk_pred)    \n",
        "    fig, ax1 = plt.subplots(figsize=(60,40),facecolor='w')\n",
        "    plt.imshow(msk_pred)\n",
        "    # --- image and predicted masks (end) ----    \n",
        "    \n",
        "\n",
        "def remove_bad_pixels_from_mask(msk):\n",
        "    kernel = np.ones((5,5))\n",
        "    msk_new = cv2.morphologyEx(msk, cv2.MORPH_OPEN, kernel)\n",
        "    return msk_new\n",
        "\n",
        "def predict_image(img_id):\n",
        "    img = tiff.imread(Data_Folder+'/input_data/{}.tif'.format(img_id))\n",
        "    img = stretch_n(img)\n",
        "    model = model_load_spacenet()\n",
        "    msk_pred = predict_id_spacenet(img_id, model)\n",
        "    msk_pred = np.squeeze(msk_pred)\n",
        "    msk_pred = np.round(msk_pred, 0)\n",
        "    msk_pred = remove_bad_pixels_from_mask(msk_pred)\n",
        "    msk_count = calculate_mask_count(msk_pred)\n",
        "    return msk_count \n",
        "\n",
        "def predict_test_image_spacenet(img, model):\n",
        "    ISZ_ = int(ISZ)\n",
        "    xx = int((img.shape[0]/ISZ_)-1)\n",
        "    yy = int((img.shape[1]/ISZ_)-1)\n",
        "    prd = np.zeros((N_Cls, img.shape[0], \n",
        "                           img.shape[1])).astype(np.float32)\n",
        "                    \n",
        "                   \n",
        "    for i in range(0, int(img.shape[0]/ISZ_)):\n",
        "        line = []\n",
        "        for j in range(0, int(img.shape[1]/ISZ_)):\n",
        "            x1 = i * ISZ_\n",
        "            x2 = (i + 1) * ISZ_\n",
        "            y1 = j * ISZ_\n",
        "            y2 = (j + 1) * ISZ_\n",
        "            line.append(img[x1:x2, y1:y2])\n",
        "            if i < xx and j == yy:\n",
        "                y1 = img.shape[1]-ISZ_\n",
        "                y2 = img.shape[1]\n",
        "                line.append(img[x1:x2, y1:y2])\n",
        "                \n",
        "        x = 2 * np.transpose(line, (0, 3, 1, 2)) - 1\n",
        "        tmp = model.predict(x, batch_size=4)\n",
        "        if len(tmp)>img.shape[1]/ISZ:\n",
        "            last = tmp[-1]\n",
        "            tmp = tmp[:-1]\n",
        "        for j in range(tmp.shape[0]):\n",
        "            prd[:, i * ISZ_:(i + 1) * ISZ_, j * ISZ_:(j + 1) * ISZ_] = tmp[j]\n",
        "            if i < xx and j == yy:\n",
        "                prd[:, i * ISZ_:(i + 1) * ISZ_, img.shape[1]-ISZ_:img.shape[1]] = last\n",
        "                \n",
        "                \n",
        "        if i == xx:\n",
        "            line = []\n",
        "            for j in range(0, int(img.shape[1]/ISZ_)):\n",
        "                x1 = img.shape[0]-ISZ_\n",
        "                x2 = img.shape[0]\n",
        "                y1 = j * ISZ_\n",
        "                y2 = (j + 1) * ISZ_\n",
        "                line.append(img[x1:x2, y1:y2])\n",
        "                if j==yy:\n",
        "                    y1 = img.shape[1]-ISZ_\n",
        "                    y2 = img.shape[1]\n",
        "                    line.append(img[x1:x2, y1:y2])\n",
        "            x = 2 * np.transpose(line, (0, 3, 1, 2)) - 1\n",
        "            tmp = model.predict(x, batch_size=4)\n",
        "            if len(tmp)>img.shape[1]/ISZ:\n",
        "                last = tmp[-1]\n",
        "                tmp = tmp[:-1]\n",
        "            for j in range(tmp.shape[0]): #\n",
        "                prd[:, img.shape[0]-ISZ_:img.shape[0], j * ISZ_:(j + 1) * ISZ_] = tmp[j]\n",
        "                if j == yy:\n",
        "                    prd[:, img.shape[0]-ISZ_:img.shape[0], img.shape[1]-ISZ_:img.shape[1]] = last\n",
        "                \n",
        "                \n",
        "    return prd[:, :img.shape[0], :img.shape[1]]\n",
        "\n",
        "@timeit\n",
        "def model_predict(model,x):\n",
        "    tmp = model.predict(x, batch_size=128)\n",
        "    return tmp\n",
        "\n",
        "#@timeit\n",
        "def predict_test_image_spacenet_sliding(img, model):\n",
        "    ISZ_ = int(ISZ)\n",
        "    step = int(ISZ_/2)\n",
        "    xx = int((img.shape[0]/step)-2)\n",
        "    yy = int((img.shape[1]/step)-2)\n",
        "    prd = np.zeros((N_Cls, img.shape[0], \n",
        "                           img.shape[1])).astype(np.float32)\n",
        "\n",
        "    for i in range(0, int( img.shape[0]/step )-1):\n",
        "        line = []\n",
        "        for j in range(0, int( img.shape[1]/step )-1):\n",
        "            x1 = i * step\n",
        "            x2 = (i + 2) * step\n",
        "            y1 = j * step\n",
        "            y2 = (j + 2) * step\n",
        "            line.append(img[x1:x2, y1:y2])\n",
        "            if i < xx and j == yy:\n",
        "                y1 = img.shape[1]-ISZ_\n",
        "                y2 = img.shape[1]\n",
        "                line.append(img[x1:x2, y1:y2])\n",
        "                \n",
        "        x = 2 * np.transpose(line, (0, 3, 1, 2)) - 1\n",
        "        tmp = model_predict(model, x)\n",
        "        print('single')\n",
        "        tmpzzz = model_predict(model, x[0:1,:,:,:])\n",
        "        if len(tmp)>img.shape[1]/step-1:\n",
        "            last = tmp[-1]\n",
        "            tmp = tmp[:-1]\n",
        "        for j in range(tmp.shape[0]): #\n",
        "            part1 = np.round(tmp[j],0) > 0\n",
        "            part2 = np.round(prd[:, i * step:(i + 2) * step, j * step:(j + 2) * step],0) > 0 \n",
        "            values = (part1 | part2).astype(np.float32)\n",
        "            prd[:, i * step:(i + 2) * step, j * step:(j + 2) * step] = values\n",
        "            if i < xx and j == yy:\n",
        "                part1 = np.round(last,0) > 0\n",
        "                part2 = np.round(prd[:, i * step:(i + 2) * step, img.shape[1]-ISZ_:img.shape[1]],0) > 0\n",
        "                values = (part1 | part2).astype(np.float32)\n",
        "                prd[:, i * step:(i + 2) * step, img.shape[1]-ISZ_:img.shape[1]] = values\n",
        "                \n",
        "                \n",
        "        if i == xx:\n",
        "            line = []\n",
        "            for j in range(0, int(img.shape[1]/step)-1):\n",
        "                x1 = img.shape[0]-ISZ_\n",
        "                x2 = img.shape[0]\n",
        "                y1 = j * step\n",
        "                y2 = (j + 2) * step\n",
        "                line.append(img[x1:x2, y1:y2])\n",
        "                if j==yy:\n",
        "                    y1 = img.shape[1]-ISZ_\n",
        "                    y2 = img.shape[1]\n",
        "                    line.append(img[x1:x2, y1:y2])\n",
        "            x = 2 * np.transpose(line, (0, 3, 1, 2)) - 1\n",
        "            tmp = model_predict(model, x)\n",
        "            if len(tmp)>img.shape[1]/step-1:\n",
        "                last = tmp[-1]\n",
        "                tmp = tmp[:-1]\n",
        "            for j in range(tmp.shape[0]): \n",
        "                part1 = np.round(tmp[j],0) > 0\n",
        "                part2 = np.round(prd[:, img.shape[0]-ISZ_:img.shape[0], j * step:(j + 2) * step],0) > 0 \n",
        "                values = (part1 | part2).astype(np.float32)\n",
        "                prd[:, img.shape[0]-ISZ_:img.shape[0], j * step:(j + 2) * step] = values\n",
        "                if j == yy:\n",
        "                    part1 = np.round(last,0) > 0\n",
        "                    part2 = np.round(prd[:, img.shape[0]-ISZ_:img.shape[0], img.shape[1]-ISZ_:img.shape[1]],0) > 0\n",
        "                    values = (part1 | part2).astype(np.float32)                    \n",
        "                    prd[:, img.shape[0]-ISZ_:img.shape[0], img.shape[1]-ISZ_:img.shape[1]] = values\n",
        "                \n",
        "                \n",
        "    return prd[:, :img.shape[0], :img.shape[1]]\n",
        "\n",
        "\n",
        "def compare_and_plot(img, msk_true, msk_pred, img_id='', save_to_disk=False):\n",
        "    blues = 0\n",
        "    img_and_msk_pred = img\n",
        "    for i in range(img.shape[0]): # for every pixel:\n",
        "        for j in range(img.shape[1]):\n",
        "            if msk_pred[i,j] == 1 and msk_true[i,j] == 1:\n",
        "                img_and_msk_pred[i,j] = (1, 1, 1)     # white\n",
        "            elif msk_pred[i,j] == 0 and msk_true[i,j] == 1:    \n",
        "                img_and_msk_pred[i,j] = (1, 0.1, 0.1) # red\n",
        "            elif msk_pred[i,j] == 1 and msk_true[i,j] == 0:    \n",
        "                img_and_msk_pred[i,j] = (0.1, 0.1, 1) # blue\n",
        "                blues = blues + 1\n",
        "    show_save_image(img_and_msk_pred, str(blues)+\"_\"+str(img_id), save_to_disk)           \n",
        "\n",
        "\n",
        "def show_save_image(img, img_name=None, save_to_disk=False):        \n",
        "    fig, ax1 = plt.subplots(figsize=(130,130),facecolor='w')\n",
        "    if save_to_disk:\n",
        "        directory = \"AOI_train_125/predicted_data_v4/\"+version_\n",
        "        if img_name:\n",
        "            file_name = \"{directory}/{name}_machine_masks_img.png\".format(name=img_name,\n",
        "                                                              directory=directory)\n",
        "        else:\n",
        "            file_name = \"{directory}/machine_masks_img.png\".format(directory=directory)\n",
        "        plt.imshow(img)\n",
        "        fig.savefig(file_name, bbox_inches='tight')\n",
        "        plt.close(fig)    \n",
        "    else:\n",
        "        plt.imshow(img)\n",
        "        \n",
        "def test_training_set():\n",
        "    \n",
        "    model = model_load_spacenet()\n",
        "    img = np.load(Big_x % N_Cls + \".npy\")[:img_size*5,:img_size*5]\n",
        "    msk_true = np.load(Big_y % N_Cls + \".npy\")[:img_size*5,:img_size*5]\n",
        "    \n",
        "    msk_true = msk_true[:,:,0]\n",
        "    msk_true = np.round(msk_true, 0)\n",
        "\n",
        "    true_msk_count_no_thres, true_msk_count = calculate_mask_count(msk_true)\n",
        "    print('Number of true masks before threshold: ',true_msk_count_no_thres) \n",
        "    print('Number of true masks after threshold: ',true_msk_count)\n",
        "\n",
        "    msk_pred = predict_test_image_spacenet(img, model)\n",
        "    msk_pred = np.squeeze(msk_pred) \n",
        "    msk_pred = np.round(msk_pred, 0)\n",
        "    pred_msk_count_no_thres, pred_msk_count = calculate_mask_count(msk_pred)\n",
        "    print('Number of prediction masks before threshold: ', pred_msk_count_no_thres) \n",
        "    print('Number of prediction masks after threshold: ', pred_msk_count)\n",
        "\n",
        "    rez = jaccard_score(msk_true, msk_pred)\n",
        "    print (\"Jaccard koef from lib:\", rez)\n",
        "    \n",
        "    \n",
        "    rez2 = jaccard_coef(msk_true, msk_pred)\n",
        "    with tf.Session() as sess:\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        print(\"Our f-tion Jaccard koef  \", rez2.eval())\n",
        "\n",
        "    print (\"Searching masks on Prediction Big Mask\")\n",
        "    true_msk_count_no_thres2, true_msk_count2 = compare_every_mask(msk_pred, msk_true)\n",
        "    print (\"Reversed. Searching masks on True Big Mask\")\n",
        "    true_msk_count_no_thres2, true_msk_count2 = compare_every_mask(msk_true, msk_pred)\n",
        "\n",
        "#@timeit\n",
        "def test_model():\n",
        "    model = model_load_spacenet()\n",
        "    img = np.load(Big_test_x % N_Cls + \".npy\")\n",
        "    msk_true = np.load(Big_test_y % N_Cls + \".npy\")\n",
        "    msk_true = msk_true[:,:,0]\n",
        "    tf_msk_true = tf.convert_to_tensor(msk_true, np.float64)\n",
        "    msk_true = np.round(msk_true, 0)\n",
        "\n",
        "    true_msk_count_no_thres, true_msk_count = calculate_mask_count(msk_true)\n",
        "    print('Number of true masks before threshold: ',true_msk_count_no_thres) \n",
        "    print('Number of true masks after threshold: ',true_msk_count)\n",
        "\n",
        "    msk_pred = predict_test_image_spacenet_sliding(img, model)\n",
        "\n",
        "    msk_pred = np.squeeze(msk_pred)\n",
        "    tf_msk_pred = tf.convert_to_tensor(msk_pred, np.float64)\n",
        "    msk_pred = np.round(msk_pred, 0)\n",
        "    msk_pred = remove_bad_pixels_from_mask(msk_pred)\n",
        "    pred_msk_count_no_thres, pred_msk_count = calculate_mask_count(msk_pred)\n",
        "    print('Number of prediction masks before threshold: ', pred_msk_count_no_thres) \n",
        "    print('Number of prediction masks after threshold: ', pred_msk_count)\n",
        "\n",
        "    rez = jaccard_score(msk_true, msk_pred)\n",
        "    print (\"Jaccard koef from lib:\", rez)\n",
        "\n",
        "    rez2 = jaccard_coef(msk_true, msk_pred)\n",
        "    with tf.Session() as sess:\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        print(\"Our f-tion Jaccard koef  \", rez2.eval())\n",
        "    rez3 = jaccard_coef_int(tf_msk_true, tf_msk_pred)\n",
        "    with tf.Session() as sess:\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        print(\"Our f-tion Jaccard koef int \", rez3.eval())\n",
        "    \n",
        "    print (\"Searching masks on Prediction Big Mask\")\n",
        "    true_msk_count_no_thres2, true_msk_count2 = compare_every_mask(msk_pred, msk_true)\n",
        "    print (\"Reversed. Searching masks on True Big Mask\")\n",
        "    true_msk_count_no_thres2, true_msk_count2 = compare_every_mask(msk_true, msk_pred)\n",
        "    \n",
        "def save_failed_images(img, model, mask_true):\n",
        "    ISZ_ = int(ISZ)\n",
        "    xx = int((img.shape[0]/ISZ_)-1)\n",
        "    yy = int((img.shape[1]/ISZ_)-1)\n",
        "    \n",
        "    for i in range(0, int(img.shape[0]/ISZ_)):\n",
        "        line = []\n",
        "        for j in range(0, int(img.shape[1]/ISZ_)):\n",
        "            x1 = i * ISZ_\n",
        "            x2 = (i + 1) * ISZ_\n",
        "            y1 = j * ISZ_\n",
        "            y2 = (j + 1) * ISZ_\n",
        "            small_img = img[x1:x2, y1:y2]\n",
        "            line.append(small_img)\n",
        "            if i < xx and j == yy:\n",
        "                y1 = img.shape[1]-ISZ_\n",
        "                y2 = img.shape[1]\n",
        "                line.append(img[x1:x2, y1:y2])\n",
        "                \n",
        "        x = 2 * np.transpose(line, (0, 3, 1, 2)) - 1\n",
        "        tmp = model.predict(x, batch_size=4)\n",
        "        if len(tmp)>img.shape[1]/ISZ:\n",
        "            last = tmp[-1]\n",
        "            tmp = tmp[:-1]\n",
        "        for j in range(tmp.shape[0]): #\n",
        "            img_id = \"i_{i}_j_{j}_\".format(i=i, j=j)\n",
        "            s_img = img[i * ISZ_:(i + 1) * ISZ_, j * ISZ_:(j + 1) * ISZ_]\n",
        "            s_mask = mask_true[i * ISZ_:(i + 1) * ISZ_, j * ISZ_:(j + 1) * ISZ_]\n",
        "            t_mask = tmp[j][0,:,:]\n",
        "            t_mask = np.round(t_mask, 0)\n",
        "            jacc = jaccard_score(s_mask, t_mask)\n",
        "            \n",
        "            if jacc < 0.6:\n",
        "                print(img_id, jacc)\n",
        "                img_id = img_id + \"_jacc_\" + \"{0:.4f}\".format(jacc) \n",
        "                img_id2 = img_id + \"_real_\"\n",
        "                show_save_image(s_img, img_id2, save_to_disk=True) \n",
        "                compare_and_plot(s_img, s_mask, t_mask, img_id=img_id, save_to_disk=True)\n",
        "            if i < xx and j == yy:\n",
        "                img_id = \"i_{i}_j_{j}_last_\".format(i=i, j=j)\n",
        "                s_img = img[i * ISZ_:(i + 1) * ISZ_, img.shape[1]-ISZ_:img.shape[1]]\n",
        "                s_mask = mask_true[i * ISZ_:(i + 1) * ISZ_, img.shape[1]-ISZ_:img.shape[1]]\n",
        "                t_mask = last[0,:,:]\n",
        "                t_mask = np.round(t_mask, 0)\n",
        "                jacc = jaccard_score(s_mask, t_mask)\n",
        "                if jacc < 0.6:\n",
        "                    img_id = img_id + \"_jacc_\" + \"{0:.4f}\".format(jacc)\n",
        "                    img_id2 = img_id + \"_real_\"\n",
        "                    show_save_image(s_img, img_id2, save_to_disk=True)\n",
        "                    compare_and_plot(s_img, s_mask, t_mask, img_id=img_id, save_to_disk=True)\n",
        "                    \n",
        "                    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wJlOVJWJanI"
      },
      "source": [
        "## Store patches to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTIX8ktgJanI",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    img = np.load(Big_x % N_Cls +\".npy\")\n",
        "    msk = np.load(Big_y % N_Cls +\".npy\")\n",
        "  \n",
        "    x_trn, y_trn = get_patches_spacenet(img, msk, amt=25000, show_image=False) \n",
        "    np.savez(patches_file_x, x=x_trn)\n",
        "    np.savez(patches_file_y, y=y_trn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfAwkQ1-JanI"
      },
      "source": [
        "## Combine all images into 1 BIG image and save it to disk \n",
        "(performe only once for new images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfhCnvV_JanJ"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    stick_all_train_spacenet_v2()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU_KkqygJanJ"
      },
      "source": [
        "## Create training set\n",
        "(We randomly take small images from 1 BIG image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Pk5ApSTJanJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    make_val_spacenet()\n",
        "    print('done')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UeWq46JanJ"
      },
      "source": [
        "## Train model and save it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cROmgOtFJanJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = train_net_spacenet()\n",
        "model_save_spacenet(model)\n",
        "print('done')  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQs0ftMlJanO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('ml_shit')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "b95ac56969846026f7a2619c3615e1e83a67c86eabba5da6141cfc614ab49c26"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
